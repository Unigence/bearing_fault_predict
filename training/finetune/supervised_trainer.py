"""
æœ‰ç›‘ç£è®­ç»ƒå™¨
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from typing import Dict, Any, Tuple, Optional
from tqdm import tqdm
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from training.trainer_base import TrainerBase
from losses import CombinedLoss, ProgressiveCombinedLoss, compute_class_weights
from augmentation.mixup import MultiModalMixup, ManifoldMixup


class SupervisedTrainer(TrainerBase):
    """æœ‰ç›‘ç£è®­ç»ƒå™¨"""

    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        optimizer: torch.optim.Optimizer,
        scheduler: Any,
        loss_config: Dict[str, Any],
        device: str = 'cuda',
        experiment_dir: str = 'experiments/runs',
        use_amp: bool = False,
        mixup_config: Optional[Dict[str, Any]] = None,
        gradient_clip_max_norm: float = 1.0
    ):
        """
        Args:
            model: æ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            optimizer: ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
            loss_config: æŸå¤±å‡½æ•°é…ç½®
            device: è®­ç»ƒè®¾å¤‡
            experiment_dir: å®éªŒä¿å­˜ç›®å½•
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            mixup_config: Mixupé…ç½®å­—å…¸ï¼ŒåŒ…å«time_domain, frequency_domain, feature_levelé…ç½®
            gradient_clip_max_norm: æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°
        """
        super().__init__(
            model, train_loader, val_loader, optimizer, scheduler,
            device, experiment_dir, use_amp
        )

        self.gradient_clip_max_norm = gradient_clip_max_norm

        # åˆ›å»ºæŸå¤±å‡½æ•°
        self.criterion = self._create_criterion(loss_config)

        # åˆ›å»ºMixupç®¡ç†å™¨
        self.mixup_manager = None
        if mixup_config:
            self.mixup_manager = MultiModalMixup(
                time_domain_config=mixup_config.get('time_domain'),
                frequency_domain_config=mixup_config.get('frequency_domain'),
                feature_level_config=mixup_config.get('feature_level')
            )

        print(f"SupervisedTraineråˆå§‹åŒ–å®Œæˆ")
        if self.mixup_manager:
            print(f"  - æ—¶åŸŸMixup: {self.mixup_manager.time_domain_enabled}")
            print(f"  - é¢‘åŸŸMixup: {self.mixup_manager.frequency_domain_enabled}")
            print(f"  - ç‰¹å¾å±‚Mixup: {self.mixup_manager.feature_level_enabled}")
            print(f"  âš ï¸  æ³¨æ„: ä½¿ç”¨è¾“å…¥å±‚Mixupæ—¶å°†ç¦ç”¨ArcFaceæŸå¤±(ä»…ä½¿ç”¨Softmax)")
        else:
            print(f"  - Mixup: æœªå¯ç”¨")
        print(f"  - æ¢¯åº¦è£å‰ª: {gradient_clip_max_norm}")

    def _create_criterion(self, loss_config: Dict[str, Any]):
        """
        åˆ›å»ºæŸå¤±å‡½æ•°

        Args:
            loss_config: æŸå¤±å‡½æ•°é…ç½®

        Returns:
            criterion: æŸå¤±å‡½æ•°å®ä¾‹
        """
        if loss_config.get('use_progressive', False):
            # æ¸è¿›å¼ç»„åˆæŸå¤±
            criterion = ProgressiveCombinedLoss(
                focal_alpha=loss_config['focal'].get('alpha', None),
                focal_gamma_init=loss_config['focal'].get('gamma_init', None),
                focal_gamma_min=loss_config['focal'].get('gamma_min', None),
                arcface_weight_init=loss_config['arcface'].get('weight_init', None),
                arcface_weight_max=loss_config['arcface'].get('weight_max', None),
                label_smoothing=loss_config.get('label_smoothing', 0.0)
            )
        else:
            # å›ºå®šæƒé‡ç»„åˆæŸå¤±
            criterion = CombinedLoss(
                focal_alpha=loss_config['focal'].get('alpha', None),
                focal_gamma=loss_config['focal'].get('gamma_init', None),
                focal_weight=loss_config['focal'].get('weight', None),
                arcface_weight=loss_config['arcface'].get('weight_init', 0.5),
                label_smoothing=loss_config.get('label_smoothing', 0.0)
            )

        return criterion

    def train_epoch(self) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch

        Returns:
            metrics: è®­ç»ƒæŒ‡æ ‡å­—å…¸ {'train_loss': ..., 'train_acc': ...}
        """
        self.model.train()

        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        pbar = tqdm(self.train_loader, desc=f"Training Epoch {self.current_epoch+1}")

        for batch_idx, batch in enumerate(pbar):
            # åº”ç”¨è¾“å…¥å±‚mixupï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.mixup_manager:
                mixed_batch, labels_a, labels_b, lam = self.mixup_manager.apply_input_mixup(
                    batch, self.device
                )
                use_mixup = labels_b is not None
            else:
                mixed_batch = batch
                labels_a = batch['label'].to(self.device)
                labels_b = None
                lam = 1.0
                use_mixup = False

            # å‰å‘ä¼ æ’­ - æ ¹æ®æ˜¯å¦ä½¿ç”¨Mixupå†³å®šè®¡ç®—æ–¹å¼
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    softmax_logits, arcface_logits, features = self.model(
                        mixed_batch, mode='supervised'
                    )

                    #  Mixupæ—¶åªä½¿ç”¨SoftmaxæŸå¤±,ä¸ä½¿ç”¨ArcFace
                    if use_mixup:
                        # Mixupæƒ…å†µ: åªç”¨Softmaxåˆ†ç±»æŸå¤±
                        # åŸå› : Mixupåçš„ç‰¹å¾ä¸åœ¨ä»»ä½•ç±»çš„æµå½¢ä¸Š,æ— æ³•è®¡ç®—æœ‰æ„ä¹‰çš„è§’åº¦è¾¹ç•Œ
                        loss_a = F.cross_entropy(softmax_logits, labels_a, reduction='mean')
                        loss_b = F.cross_entropy(softmax_logits, labels_b, reduction='mean')
                        loss = lam * loss_a + (1 - lam) * loss_b
                    else:
                        # æ­£å¸¸æƒ…å†µ: ä½¿ç”¨å®Œæ•´çš„ç»„åˆæŸå¤±(Focal + ArcFace)
                        loss, loss_dict = self.criterion(softmax_logits, arcface_logits, labels_a)
            else:
                softmax_logits, arcface_logits, features = self.model(
                    mixed_batch, mode='supervised'
                )

                if use_mixup:
                    loss_a = F.cross_entropy(softmax_logits, labels_a, reduction='mean')
                    loss_b = F.cross_entropy(softmax_logits, labels_b, reduction='mean')
                    loss = lam * loss_a + (1 - lam) * loss_b
                else:
                    loss, loss_dict = self.criterion(softmax_logits, arcface_logits, labels_a)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()

            if self.use_amp:
                self.scaler.scale(loss).backward()
                # æ¢¯åº¦è£å‰ª
                if self.gradient_clip_max_norm > 0:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.gradient_clip_max_norm
                    )
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                if self.gradient_clip_max_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.gradient_clip_max_norm
                    )
                self.optimizer.step()

            # ç»Ÿè®¡å‡†ç¡®ç‡
            with torch.no_grad():
                predictions = torch.argmax(softmax_logits, dim=1)

                if use_mixup:
                    # Mixupæƒ…å†µï¼šä½¿ç”¨ç¡¬æ ‡ç­¾è®¡ç®—å‡†ç¡®ç‡ï¼ˆå–lambdaè¾ƒå¤§çš„é‚£ä¸ªï¼‰
                    if lam > 0.5:
                        correct = (predictions == labels_a).sum().item()
                    else:
                        correct = (predictions == labels_b).sum().item()
                else:
                    correct = (predictions == labels_a).sum().item()

                total_correct += correct
                total_samples += labels_a.size(0)

            # ç»Ÿè®¡loss
            total_loss += loss.item()
            self.global_step += 1

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': loss.item(),
                'acc': total_correct / total_samples,
                'mixup': use_mixup
            })

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / len(self.train_loader)
        avg_acc = total_correct / total_samples

        return {
            'train_loss': avg_loss,
            'train_acc': avg_acc
        }

    def validate_epoch(self) -> Dict[str, float]:
        """
        éªŒè¯ä¸€ä¸ªepoch

        ğŸ”§ ä¿®å¤2: éªŒè¯æ—¶ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æŸå¤±å‡½æ•°è®¡ç®—

        Returns:
            metrics: éªŒè¯æŒ‡æ ‡å­—å…¸ {'val_loss': ..., 'val_acc': ...}
        """
        self.model.eval()

        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        pbar = tqdm(self.val_loader, desc=f"Validation Epoch {self.current_epoch+1}")

        with torch.no_grad():
            for batch in pbar:
                labels = batch['label'].to(self.device)

                # ğŸ”§ å…³é”®ä¿®å¤: éªŒè¯æ—¶ä¹Ÿä½¿ç”¨åŒå¤´è¾“å‡ºå’Œå®Œæ•´æŸå¤±
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        # éªŒè¯æ¨¡å¼ä»ç„¶è¿”å›åŒå¤´è¾“å‡º
                        softmax_logits, arcface_logits, features = self.model(
                            batch, mode='supervised'
                        )

                        # ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æŸå¤±å‡½æ•°ï¼ˆä½†ä¸ä½¿ç”¨Mixupï¼‰
                        loss, loss_dict = self.criterion(softmax_logits, arcface_logits, labels)
                else:
                    softmax_logits, arcface_logits, features = self.model(
                        batch, mode='supervised'
                    )
                    loss, loss_dict = self.criterion(softmax_logits, arcface_logits, labels)

                # ç»Ÿè®¡å‡†ç¡®ç‡ - ä½¿ç”¨softmax_logits
                predictions = torch.argmax(softmax_logits, dim=1)
                correct = (predictions == labels).sum().item()

                total_correct += correct
                total_samples += labels.size(0)
                total_loss += loss.item()

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': loss.item(),
                    'acc': total_correct / total_samples
                })

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / len(self.val_loader)
        avg_acc = total_correct / total_samples

        return {
            'val_loss': avg_loss,
            'val_acc': avg_acc
        }

    def compute_loss(self, batch: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        è®¡ç®—æŸå¤±ï¼ˆç”¨äºå…¶ä»–ç”¨é€”ï¼Œå¦‚åˆ†æï¼‰

        Args:
            batch: è¾“å…¥batch

        Returns:
            loss: æ€»æŸå¤±
            loss_dict: æŸå¤±å­—å…¸
        """
        labels = batch['label'].to(self.device)

        # å‰å‘ä¼ æ’­
        softmax_logits, arcface_logits, features = self.model(batch, mode='supervised')

        # è®¡ç®—æŸå¤±
        loss, loss_dict = self.criterion(softmax_logits, arcface_logits, labels)

        return loss, loss_dict

    def freeze_backbone(self, freeze_ratio: float = 0.5):
        """
        å†»ç»“éƒ¨åˆ†backboneå‚æ•°

        Args:
            freeze_ratio: å†»ç»“æ¯”ä¾‹(0.0-1.0)
        """
        print(f"\nå†»ç»“ {freeze_ratio*100:.0f}% çš„backboneå‚æ•°")
        self.model.freeze_backbone(freeze_ratio=freeze_ratio)

    def unfreeze_all(self):
        """è§£å†»æ‰€æœ‰å‚æ•°"""
        print("\nè§£å†»æ‰€æœ‰å‚æ•°")
        self.model.unfreeze_all()

    def update_epoch(self, epoch: int, max_epochs: int):
        """
        æ›´æ–°epochï¼ˆç”¨äºæ¸è¿›å¼æŸå¤±ï¼‰

        Args:
            epoch: å½“å‰epoch
            max_epochs: æœ€å¤§epochæ•°
        """
        if hasattr(self.criterion, 'update_epoch'):
            self.criterion.update_epoch(epoch, max_epochs)


if __name__ == '__main__':
    """æµ‹è¯•ä»£ç """
    print("=" * 70)
    print("SupervisedTraineræµ‹è¯•ï¼ˆå·²ä¿®å¤ç‰ˆæœ¬ï¼‰")
    print("=" * 70)

    # æµ‹è¯•Mixupé…ç½®
    mixup_config = {
        'time_domain': {
            'enable': True,
            'alpha': 0.2,
            'prob': 0.5
        },
        'frequency_domain': {
            'enable': False,
            'alpha': 0.2,
            'prob': 0.3,
            'mix_mode': 'magnitude'
        },
        'feature_level': {
            'enable': False,
            'alpha': 0.2,
            'prob': 0.5
        }
    }

    print("\nMixupé…ç½®ç¤ºä¾‹:")
    print(f"  - æ—¶åŸŸ: {mixup_config['time_domain']['enable']}")
    print(f"  - é¢‘åŸŸ: {mixup_config['frequency_domain']['enable']}")
    print(f"  - ç‰¹å¾å±‚: {mixup_config['feature_level']['enable']}")
    print("\nâœ… ä¿®å¤è¯´æ˜:")
    print("  1. ä½¿ç”¨è¾“å…¥å±‚Mixupæ—¶,åªä½¿ç”¨SoftmaxæŸå¤±")
    print("  2. éªŒè¯æ—¶ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„å®Œæ•´æŸå¤±å‡½æ•°")

    print("\nâœ“ SupervisedTraineræ¨¡å—åŠ è½½æˆåŠŸ")
    print("=" * 70)
