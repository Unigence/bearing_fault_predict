"""
å¾®è°ƒå¯åŠ¨è„šæœ¬
"""
import torch
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import create_model
from datasets import BearingDataset
from torch.utils.data import DataLoader
from augmentation import get_augmentation_pipeline
from training.finetune.supervised_trainer import SupervisedTrainer
from training.optimizer_factory import create_optimizer_from_config
from training.scheduler_factory import create_scheduler_from_config
from utils.config_parser import ModelConfigParser, TrainConfigParser, AugmentationConfigParser
from utils.seed import set_seed


def create_supervised_dataloaders(
    data_config: dict,
    aug_config: AugmentationConfigParser,
    batch_size: int,
    max_epochs: int
):
    """
    åˆ›å»ºæœ‰ç›‘ç£æ•°æ®åŠ è½½å™¨

    Args:
        data_config: æ•°æ®é…ç½®
        aug_config: å¢å¼ºé…ç½®è§£æå™¨
        batch_size: batchå¤§å°
        max_epochs: æœ€å¤§è®­ç»ƒè½®æ•°(ç”¨äºæ¸è¿›å¼å¢å¼º)

    Returns:
        train_loader, val_loader
    """
    # åˆ›å»ºè®­ç»ƒå¢å¼º
    # ä½¿ç”¨æ¸è¿›å¼å¢å¼ºæˆ–æ’å®šå¢å¼º
    if aug_config.use_progressive() and not aug_config.use_constant():
        # æ¸è¿›å¼å¢å¼º(epoch=0æ—¶ä½¿ç”¨weak)
        train_augmentation = get_augmentation_pipeline(
            stage='supervised',
            epoch=0,
            max_epochs=max_epochs,
            mode='train'
        )
        use_progressive = True
    else:
        # æ’å®šå¢å¼º
        intensity = aug_config.get_default_intensity()
        train_augmentation = get_augmentation_pipeline(
            stage='supervised',
            epoch=int(max_epochs * 0.5),  # ä½¿ç”¨ä¸­ç­‰epochå¯¹åº”çš„å¼ºåº¦
            max_epochs=max_epochs,
            mode='train'
        )
        use_progressive = False

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = BearingDataset(
        data_dir=data_config.get('train_dir', 'raw_datasets/train'),
        mode='train',
        fold=data_config.get('current_fold', 0),
        n_folds=data_config.get('n_folds', 5),
        window_size=data_config.get('window_size', 512),
        window_step=data_config.get('window_step', 256),
        timefreq_method=data_config.get('timefreq_method', 'stft'),
        augmentation=train_augmentation,
        cache_data=data_config.get('cache_data', True)
    )

    # éªŒè¯é›†ä¸ä½¿ç”¨å¢å¼º
    val_dataset = BearingDataset(
        data_dir=data_config.get('train_dir', 'raw_datasets/train'),
        mode='val',
        fold=data_config.get('current_fold', 0),
        n_folds=data_config.get('n_folds', 5),
        window_size=data_config.get('window_size', 512),
        window_step=data_config.get('window_step', 512),
        timefreq_method=data_config.get('timefreq_method', 'stft'),
        augmentation=None,  # éªŒè¯é›†ä¸å¢å¼º
        cache_data=data_config.get('cache_data', True)
    )

    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=data_config.get('num_workers', 4),
        pin_memory=data_config.get('pin_memory', True),
        drop_last=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=data_config.get('num_workers', 4),
        pin_memory=data_config.get('pin_memory', True)
    )

    return train_loader, val_loader, use_progressive


class ProgressiveAugmentationTrainer(SupervisedTrainer):
    """
    ç»§æ‰¿è‡ªSupervisedTrainer,æ·»åŠ æ¯ä¸ªepochæ›´æ–°augmentationçš„åŠŸèƒ½
    """

    def __init__(
        self,
        *args,
        use_progressive_aug: bool = False,
        max_epochs: int = 100,
        **kwargs
    ):
        """
        Args:
            use_progressive_aug: æ˜¯å¦ä½¿ç”¨æ¸è¿›å¼å¢å¼º
            max_epochs: æœ€å¤§epochæ•°
        """
        super().__init__(*args, **kwargs)
        self.use_progressive_aug = use_progressive_aug
        self.max_epochs = max_epochs

        if self.use_progressive_aug:
            print(f"  âœ… å¯ç”¨æ¸è¿›å¼å¢å¼º,å°†åœ¨æ¯ä¸ªepochæ›´æ–°augmentationå¼ºåº¦")

    def update_augmentation(self, epoch: int):
        """
        æ›´æ–°è®­ç»ƒé›†çš„augmentation

        Args:
            epoch: å½“å‰epoch
        """
        if not self.use_progressive_aug:
            return

        # åˆ›å»ºæ–°çš„augmentation pipeline
        new_augmentation = get_augmentation_pipeline(
            stage='supervised',
            epoch=epoch,
            max_epochs=self.max_epochs,
            mode='train'
        )

        # æ›´æ–°datasetçš„augmentation
        self.train_loader.dataset.augmentation = new_augmentation

        # æ‰“å°å½“å‰å¢å¼ºå¼ºåº¦
        progress = epoch / self.max_epochs
        if progress < 0.3:
            intensity = "å¼±"
        elif progress < 0.7:
            intensity = "ä¸­"
        else:
            intensity = "å¼º"

        print(f"  ğŸ“Š Epoch {epoch+1}: æ›´æ–°å¢å¼ºå¼ºåº¦ -> {intensity} (progress={progress:.2f})")

    def train(
        self,
        epochs: int,
        log_interval: int = 10,
        save_config: Optional[dict] = None
    ):
        """
        é‡å†™trainæ–¹æ³•,åœ¨æ¯ä¸ªepochå¼€å§‹å‰æ›´æ–°augmentation

        Args:
            epochs: è®­ç»ƒè½®æ•°
            log_interval: æ—¥å¿—æ‰“å°é—´éš”
            save_config: ä¿å­˜é…ç½®
        """
        print("=" * 80)
        print(f"å¼€å§‹è®­ç»ƒ: {epochs} epochs")
        if self.use_progressive_aug:
            print("  âœ… æ¸è¿›å¼å¢å¼ºå·²å¯ç”¨")
        print("=" * 80)

        # ä¿å­˜é…ç½®
        if save_config:
            self._save_config(save_config)

        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            self.current_epoch = epoch

            # æ¯ä¸ªepochå¼€å§‹å‰æ›´æ–°augmentation
            self.update_augmentation(epoch)

            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self.train_epoch()

            # éªŒè¯ä¸€ä¸ªepoch
            val_metrics = self.validate_epoch()

            # æ›´æ–°å­¦ä¹ ç‡
            self._update_lr(val_metrics)

            # æ›´æ–°æ¸è¿›å¼æŸå¤±çš„epoch
            # self.update_epoch(epoch, epochs)

            # è®°å½•æŒ‡æ ‡
            epoch_metrics = {**train_metrics, **val_metrics}
            self.metrics_tracker.update(epoch_metrics)

            # å›è°ƒ
            callback_metrics = {
                'epoch': epoch,
                'train_loss': train_metrics.get('train_loss', 0),
                'val_loss': val_metrics.get('val_loss', 0),
                'val_acc': val_metrics.get('val_acc', 0)
            }
            self.callbacks.on_epoch_end(epoch, callback_metrics, self.model, self.optimizer)

            # æ‰“å°æ—¥å¿—
            import time
            epoch_time = 0  # å¯ä»¥åœ¨å®é™…å®ç°ä¸­è®¡æ—¶
            self._print_epoch_log(epoch, epochs, train_metrics, val_metrics, epoch_time)

            # æ£€æŸ¥æ—©åœ
            if self.callbacks.should_stop():
                print(f"\næ—©åœè§¦å‘,åœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                break

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self._plot_curves()

        # è®­ç»ƒç»“æŸ
        print("\n" + "=" * 80)
        print("è®­ç»ƒå®Œæˆ!")
        print("=" * 80)


def launch_finetune(
    model_config: ModelConfigParser,
    train_config: TrainConfigParser,
    aug_config: AugmentationConfigParser,
    pretrained_weights: Optional[str] = None,
    experiment_name: str = None
):
    """
    å¯åŠ¨å¾®è°ƒ

    Args:
        model_config: æ¨¡å‹é…ç½®è§£æå™¨
        train_config: è®­ç»ƒé…ç½®è§£æå™¨
        aug_config: å¢å¼ºé…ç½®è§£æå™¨
        pretrained_weights: é¢„è®­ç»ƒæƒé‡è·¯å¾„
        experiment_name: å®éªŒåç§°

    Returns:
        finetuned_model: å¾®è°ƒåçš„æ¨¡å‹
        experiment_dir: å®éªŒç›®å½•
        best_weights_path: æœ€ä½³æƒé‡è·¯å¾„
    """
    # è®¾ç½®éšæœºç§å­
    seed = train_config.get_seed()
    set_seed(seed)

    # è·å–è®¾å¤‡
    device = train_config.get_device()
    if not torch.cuda.is_available() and device == 'cuda':
        print("âš ï¸  CUDAä¸å¯ç”¨,ä½¿ç”¨CPUè®­ç»ƒ")
        device = 'cpu'

    print("=" * 80)
    print("æœ‰ç›‘ç£å¾®è°ƒ")
    print("=" * 80)
    print(f"è®¾å¤‡: {device}")
    print(f"éšæœºç§å­: {seed}")

    # åˆ›å»ºå®éªŒç›®å½•
    if experiment_name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        experiment_name = f"finetune_{timestamp}"

    experiment_base = train_config.get('experiment.save_dir', 'experiments/runs')
    experiment_dir = Path(experiment_base) / experiment_name
    experiment_dir.mkdir(parents=True, exist_ok=True)

    print(f"å®éªŒç›®å½•: {experiment_dir}")

    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    model_params = model_config.get_model_params()
    model = create_model(**model_params, enable_contrastive=False)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if pretrained_weights and os.path.exists(pretrained_weights):
        print(f"\nåŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_weights}")
        model.load_pretrained_backbone(pretrained_weights)

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    param_dict = model.count_parameters()
    print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"  - é…ç½®: {model_params['config']}")
    print(f"  - æ€»å‚æ•°: {param_dict['total']:,}")
    print(f"  - å¯è®­ç»ƒå‚æ•°: {param_dict['trainable']:,}")

    # è·å–å¾®è°ƒé…ç½®
    finetune_params = train_config.get_finetune_params()
    data_params = train_config.get_data_params()

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨æ—¶è·å–æ˜¯å¦ä½¿ç”¨æ¸è¿›å¼å¢å¼º
    print("\nåˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, use_progressive = create_supervised_dataloaders(
        data_config=data_params,
        aug_config=aug_config,
        batch_size=finetune_params['batch_size'],
        max_epochs=finetune_params['epochs']
    )
    print(f"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    print(f"  - è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
    print(f"  - éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
    print(f"  - Batch size: {finetune_params['batch_size']}")
    if use_progressive:
        print(f"  âœ… ä½¿ç”¨æ¸è¿›å¼å¢å¼º")

    # åˆ›å»ºä¼˜åŒ–å™¨
    print("\nåˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = create_optimizer_from_config(
        model,
        finetune_params['optimizer']
    )
    print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ: {type(optimizer).__name__}")

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    print("\nåˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨...")
    scheduler, needs_metric = create_scheduler_from_config(
        optimizer,
        finetune_params['scheduler'],
        total_epochs=finetune_params['epochs']
    )
    print(f"âœ“ è°ƒåº¦å™¨åˆ›å»ºæˆåŠŸ: {type(scheduler).__name__}")

    # ä½¿ç”¨æ–°çš„ProgressiveAugmentationTrainer
    print("\nåˆ›å»ºè®­ç»ƒå™¨...")
    trainer = ProgressiveAugmentationTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        scheduler=scheduler,
        loss_config=finetune_params['loss'],
        device=device,
        experiment_dir=str(experiment_dir),
        use_amp=train_config.use_amp(),
        mixup_config=train_config.get('mixup', None),
        gradient_clip_max_norm=finetune_params['gradient_clip'].get('max_norm', 1.0),
        use_progressive_aug=use_progressive,
        max_epochs=finetune_params['epochs']
    )

    # è®¾ç½®callbacks
    trainer.setup_callbacks(
        early_stopping_config=finetune_params.get('early_stopping', {}),
        checkpoint_config=model_config.get_checkpoint_params()
    )

    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 80)
    print("å¼€å§‹å¾®è°ƒ")
    print("=" * 80)

    trainer.train(
        epochs=finetune_params['epochs'],
        log_interval=train_config.get('experiment.logging.log_interval', 10),
        save_config={
            'model': model_config.to_dict(),
            'train': train_config.to_dict(),
            'augmentation': aug_config.to_dict()
        }
    )

    # ä¿å­˜æœ€ä½³æƒé‡
    best_weights_path = experiment_dir / 'best_model.pth'

    print("\n" + "=" * 80)
    print("âœ“ å¾®è°ƒå®Œæˆ!")
    print("=" * 80)
    print(f"æœ€ä½³æƒé‡: {best_weights_path}")
    print(f"å®éªŒç›®å½•: {experiment_dir}")

    return model, experiment_dir, best_weights_path


if __name__ == '__main__':
    """ç‹¬ç«‹è¿è¡Œå¾®è°ƒ"""
    print("=" * 70)
    print("å¾®è°ƒå¯åŠ¨è„šæœ¬æµ‹è¯•ï¼ˆå·²ä¿®å¤ç‰ˆæœ¬ï¼‰")
    print("=" * 70)
