"""
å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨
ç”¨äºé¢„è®­ç»ƒé˜¶æ®µçš„è‡ªç›‘ç£å­¦ä¹ 
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from typing import Dict, Any, Tuple
from tqdm import tqdm
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from training.trainer_base import TrainerBase
from losses import NTXentLoss, SupConLoss
from training.scheduler_factory import WarmupScheduler


class ContrastiveTrainer(TrainerBase):
    """å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨"""

    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        optimizer: torch.optim.Optimizer,
        scheduler: Any,
        loss_type: str = 'ntxent',
        temperature: float = 0.07,
        device: str = 'cuda',
        experiment_dir: str = 'experiments/runs',
        use_amp: bool = False,
        gradient_clip_max_norm: float = 1.0
    ):
        """
        Args:
            model: æ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨(å¯¹æ¯”å­¦ä¹ æ•°æ®é›†)
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            optimizer: ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
            loss_type: æŸå¤±ç±»å‹ 'ntxent' | 'supcon'
            temperature: æ¸©åº¦å‚æ•°
            device: è®­ç»ƒè®¾å¤‡
            experiment_dir: å®éªŒä¿å­˜ç›®å½•
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            gradient_clip_max_norm: æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°
        """
        super().__init__(
            model, train_loader, val_loader, optimizer, scheduler,
            device, experiment_dir, use_amp
        )

        self.loss_type = loss_type
        self.temperature = temperature
        self.gradient_clip_max_norm = gradient_clip_max_norm

        # åˆ›å»ºå¯¹æ¯”å­¦ä¹ æŸå¤±
        if loss_type == 'ntxent':
            self.criterion = NTXentLoss(temperature=temperature)
        elif loss_type == 'supcon':
            self.criterion = SupConLoss(temperature=temperature)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±ç±»å‹: {loss_type}")

        # æ£€æŸ¥scheduleræ˜¯å¦éœ€è¦metric
        self.scheduler_needs_metric = self._check_scheduler_needs_metric()

        print(f"ContrastiveTraineråˆå§‹åŒ–å®Œæˆ")
        print(f"  - æŸå¤±ç±»å‹: {loss_type}")
        print(f"  - æ¸©åº¦å‚æ•°: {temperature}")
        print(f"  - æ¢¯åº¦è£å‰ª: {gradient_clip_max_norm}")
        print(f"  - Scheduleréœ€è¦metric: {self.scheduler_needs_metric}")

    def _check_scheduler_needs_metric(self) -> bool:
        """æ£€æŸ¥scheduleræ˜¯å¦éœ€è¦metricï¼ˆç”¨äºReduceLROnPlateauï¼‰"""
        if self.scheduler is None:
            return False

        # æ£€æŸ¥scheduleræœ¬èº«
        if isinstance(self.scheduler, ReduceLROnPlateau):
            return True

        # æ£€æŸ¥WarmupSchedulerçš„base_scheduler
        if isinstance(self.scheduler, WarmupScheduler):
            if self.scheduler.base_scheduler is not None:
                if isinstance(self.scheduler.base_scheduler, ReduceLROnPlateau):
                    return True

        return False

    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()

        total_loss = 0.0
        num_batches = 0

        pbar = tqdm(self.train_loader, desc='Training')
        for batch in pbar:
            # ğŸ”§ ä¿®å¤: ç¡®ä¿batchåŒ…å«view1å’Œview2
            if 'view1' not in batch or 'view2' not in batch:
                raise ValueError(
                    "å¯¹æ¯”å­¦ä¹ æ•°æ®é›†åº”è¿”å›ä¸¤ä¸ªå¢å¼ºç‰ˆæœ¬(view1, view2)\n"
                    "è¯·æ£€æŸ¥ContrastiveDataset.__getitem__æ˜¯å¦æ­£ç¡®å®ç°"
                )

            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
            view1 = {k: v.to(self.device) for k, v in batch['view1'].items()}
            view2 = {k: v.to(self.device) for k, v in batch['view2'].items()}

            # æœ‰ç›‘ç£å¯¹æ¯”å­¦ä¹ éœ€è¦æ ‡ç­¾
            labels = None
            if 'label' in batch:
                labels = batch['label'].to(self.device)

            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    # å‰å‘ä¼ æ’­ - å¯¹æ¯”å­¦ä¹ æ¨¡å¼
                    z1, z2 = self.model({'view1': view1, 'view2': view2}, mode='contrastive')

                    # è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±
                    if self.loss_type == 'ntxent':
                        loss = self.criterion(z1, z2)
                    else:  # supcon
                        if labels is None:
                            raise ValueError("SupConéœ€è¦æ ‡ç­¾,ä½†batchä¸­æ²¡æœ‰'label'")
                        loss = self.criterion(z1, z2, labels)

                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                self.scaler.scale(loss).backward()

                # æ¢¯åº¦è£å‰ª
                if self.gradient_clip_max_norm > 0:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.gradient_clip_max_norm
                    )

                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                # æ ‡å‡†è®­ç»ƒ
                z1, z2 = self.model({'view1': view1, 'view2': view2}, mode='contrastive')

                if self.loss_type == 'ntxent':
                    loss = self.criterion(z1, z2)
                else:
                    if labels is None:
                        raise ValueError("SupConéœ€è¦æ ‡ç­¾,ä½†batchä¸­æ²¡æœ‰'label'")
                    loss = self.criterion(z1, z2, labels)

                self.optimizer.zero_grad()
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                if self.gradient_clip_max_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.gradient_clip_max_norm
                    )

                self.optimizer.step()

            # ç´¯è®¡æŸå¤±
            total_loss += loss.item()
            num_batches += 1

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({'loss': loss.item()})

        avg_loss = total_loss / num_batches
        return {'loss': avg_loss}

    def validate_epoch(self) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()

        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                # ç¡®ä¿batchæ ¼å¼æ­£ç¡®
                if 'view1' not in batch or 'view2' not in batch:
                    raise ValueError("å¯¹æ¯”å­¦ä¹ æ•°æ®é›†åº”è¿”å›ä¸¤ä¸ªå¢å¼ºç‰ˆæœ¬(view1, view2)")

                # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
                view1 = {k: v.to(self.device) for k, v in batch['view1'].items()}
                view2 = {k: v.to(self.device) for k, v in batch['view2'].items()}

                labels = None
                if 'label' in batch:
                    labels = batch['label'].to(self.device)

                # å‰å‘ä¼ æ’­
                z1, z2 = self.model({'view1': view1, 'view2': view2}, mode='contrastive')

                # è®¡ç®—æŸå¤±
                if self.loss_type == 'ntxent':
                    loss = self.criterion(z1, z2)
                else:
                    if labels is None:
                        raise ValueError("SupConéœ€è¦æ ‡ç­¾,ä½†batchä¸­æ²¡æœ‰'label'")
                    loss = self.criterion(z1, z2, labels)

                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches
        return {'loss': avg_loss}

    def _update_scheduler(self, val_metrics: Dict[str, float]):
        """
        æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨

        Args:
            val_metrics: éªŒè¯é›†æŒ‡æ ‡å­—å…¸
        """
        if self.scheduler is None:
            return

        # ğŸ”§ ä¿®å¤: å¦‚æœscheduleréœ€è¦metric,ä¼ å…¥val_loss
        if self.scheduler_needs_metric:
            metric_value = val_metrics.get('loss', None)
            if metric_value is None:
                print("âš ï¸  Warning: ReduceLROnPlateauéœ€è¦metric,ä½†æœªæ‰¾åˆ°loss")
                return
            self.scheduler.step(metric_value)
        else:
            # æ™®é€šschedulerä¸éœ€è¦metric
            self.scheduler.step()

    def save_pretrained_weights(self, save_path: str):
        """
        ä¿å­˜é¢„è®­ç»ƒæƒé‡ï¼ˆä¸åŒ…æ‹¬æŠ•å½±å¤´ï¼‰

        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        # è·å–backboneæƒé‡ï¼ˆæ’é™¤projection_headï¼‰
        backbone_state = {
            k: v for k, v in self.model.state_dict().items()
            if not k.startswith('projection_head')
        }

        torch.save({
            'backbone_state_dict': backbone_state,
            'epoch': self.current_epoch,
        }, save_path)

        print(f"âœ“ é¢„è®­ç»ƒæƒé‡å·²ä¿å­˜: {save_path}")
        print(f"  (ä¸åŒ…æ‹¬projection_head)")


if __name__ == '__main__':
    """æµ‹è¯•ä»£ç """
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

    from models import create_model
    from datasets import ContrastiveDataset
    from torch.utils.data import DataLoader
    from training.optimizer_factory import create_optimizer_from_config
    from training.scheduler_factory import create_scheduler_from_config

    print("=" * 80)
    print("æµ‹è¯•ContrastiveTrainer")
    print("=" * 80)

    # åˆ›å»ºæ¨¡å‹
    model = create_model(config='small', enable_contrastive=True)
    print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")

    # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨å°‘é‡æ•°æ®æµ‹è¯•ï¼‰
    print("\nåˆ›å»ºæ•°æ®é›†...")
    train_dataset = ContrastiveDataset(
        data_dir='raw_datasets/train',
        mode='train',
        fold=0,
        n_folds=5,
        use_labels=False,
        cache_data=False
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=4,
        shuffle=True,
        num_workers=0
    )

    val_loader = DataLoader(
        train_dataset,
        batch_size=4,
        shuffle=False,
        num_workers=0
    )

    print(f"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")

    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer_config = {'type': 'adam', 'lr': 0.001}
    optimizer = create_optimizer_from_config(model, optimizer_config)

    scheduler_config = {'type': 'cosine', 'T_max': 10, 'eta_min': 1e-6}
    scheduler, needs_metric = create_scheduler_from_config(
        optimizer,
        scheduler_config,
        total_epochs=10
    )

    print(f"âœ“ ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆ›å»ºæˆåŠŸ")
    print(f"  - Scheduleréœ€è¦metric: {needs_metric}")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ContrastiveTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        scheduler=scheduler,
        loss_type='ntxent',
        temperature=0.07,
        device='cpu',
        experiment_dir='/tmp/test_contrastive',
        use_amp=False
    )

    print(f"âœ“ Traineråˆ›å»ºæˆåŠŸ")

    # æµ‹è¯•ä¸€ä¸ªepoch
    print("\næµ‹è¯•è®­ç»ƒä¸€ä¸ªepoch...")
    train_metrics = trainer.train_epoch()
    print(f"âœ“ è®­ç»ƒå®Œæˆ: loss={train_metrics['loss']:.4f}")

    print("\næµ‹è¯•éªŒè¯ä¸€ä¸ªepoch...")
    val_metrics = trainer.validate_epoch()
    print(f"âœ“ éªŒè¯å®Œæˆ: loss={val_metrics['loss']:.4f}")

    print("\n" + "=" * 80)
    print("âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 80)






