"""
å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨
"""
import torch
import torch.nn as nn
from pathlib import Path
from typing import Dict, Optional
from training.trainer_base import TrainerBase
from utils.visualization import TrainingVisualizer


class ContrastiveTrainer(TrainerBase):
    """å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨ (ä¿®å¤ç‰ˆ)"""

    def __init__(
        self,
        model: nn.Module,
        train_loader,
        val_loader,
        optimizer,
        scheduler,
        loss_type: str = 'ntxent',
        temperature: float = 0.07,
        device: str = 'cuda',
        experiment_dir: str = './experiments',
        use_amp: bool = False,
        gradient_clip_max_norm: float = 1.0
    ):
        """
        Args:
            model: æ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            optimizer: ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
            loss_type: æŸå¤±ç±»å‹ ('ntxent', 'supcon')
            temperature: æ¸©åº¦å‚æ•°
            device: è®¾å¤‡
            experiment_dir: å®éªŒç›®å½•
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦
            gradient_clip_max_norm: æ¢¯åº¦è£å‰ªæœ€å¤§norm
        """
        super().__init__(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            optimizer=optimizer,
            scheduler=scheduler,
            device=device,
            experiment_dir=experiment_dir,
            use_amp=use_amp,
            gradient_clip_max_norm=gradient_clip_max_norm
        )

        self.temperature = temperature
        self.loss_type = loss_type

        # åˆ›å»ºå¯¹æ¯”å­¦ä¹ æŸå¤±
        from losses import NTXentLoss, SupConLoss

        if loss_type == 'ntxent':
            self.criterion = NTXentLoss(temperature=temperature)
        elif loss_type == 'supcon':
            self.criterion = SupConLoss(temperature=temperature)
        else:
            raise ValueError(f"Unknown loss type: {loss_type}")

        print(f"ContrastiveTraineråˆå§‹åŒ–å®Œæˆ")
        print(f"  - æŸå¤±ç±»å‹: {loss_type}")
        print(f"  - æ¸©åº¦å‚æ•°: {temperature}")

    def train_epoch(self, epoch: int, log_interval: int = 10) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0

        for batch_idx, (view1, view2) in enumerate(self.train_loader):
            view1 = view1.to(self.device)
            view2 = view2.to(self.device)

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()

            if self.use_amp:
                with torch.cuda.amp.autocast():
                    # è·å–ä¸¤ä¸ªè§†å›¾çš„ç‰¹å¾
                    z1 = self.model(view1, return_features=True)
                    z2 = self.model(view2, return_features=True)

                    # è®¡ç®—å¯¹æ¯”æŸå¤±
                    loss = self.criterion(z1, z2)

                self.scaler.scale(loss).backward()
                if self.gradient_clip_max_norm > 0:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.gradient_clip_max_norm
                    )
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                z1 = self.model(view1, return_features=True)
                z2 = self.model(view2, return_features=True)
                loss = self.criterion(z1, z2)

                loss.backward()
                if self.gradient_clip_max_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.gradient_clip_max_norm
                    )
                self.optimizer.step()

            total_loss += loss.item()
            num_batches += 1
            self.global_step += 1

            # æ‰“å°æ—¥å¿—
            if (batch_idx + 1) % log_interval == 0:
                avg_loss = total_loss / num_batches
                lr = self.optimizer.param_groups[0]['lr']
                print(f"  Batch [{batch_idx+1}/{len(self.train_loader)}] | "
                      f"Loss: {avg_loss:.4f} | LR: {lr:.6f}")

        avg_loss = total_loss / num_batches
        return {'train_loss': avg_loss}

    def validate_epoch(self, epoch: int) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for view1, view2 in self.val_loader:
                view1 = view1.to(self.device)
                view2 = view2.to(self.device)

                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        z1 = self.model(view1, return_features=True)
                        z2 = self.model(view2, return_features=True)
                        loss = self.criterion(z1, z2)
                else:
                    z1 = self.model(view1, return_features=True)
                    z2 = self.model(view2, return_features=True)
                    loss = self.criterion(z1, z2)

                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches
        return {'val_loss': avg_loss}

    def setup_callbacks(self, early_stopping_config: Dict, checkpoint_config: Dict):
        """
        è®¾ç½®callbacks,ç¡®ä¿é¢„è®­ç»ƒç›‘æ§æ­£ç¡®çš„æŒ‡æ ‡

        å¯¹äºé¢„è®­ç»ƒ:
        - EarlyStopping ç›‘æ§ 'val_loss'
        - ModelCheckpoint ç›‘æ§ 'val_loss'
        """
        from training.callbacks import EarlyStopping, ModelCheckpoint

        # æ—©åœ
        if early_stopping_config.get('enable', True):
            monitor = early_stopping_config.get('monitor', 'val_loss')
            if monitor == 'loss':
                monitor = 'val_loss'

            early_stopping = EarlyStopping(
                patience=early_stopping_config.get('patience', 10),
                monitor=monitor,
                mode='min',  # é¢„è®­ç»ƒå›ºå®šä¸º min æ¨¡å¼
                restore_best_weights=early_stopping_config.get('restore_best_weights', True)
            )
            self.add_callback(early_stopping)
            print(f"  âœ… EarlyStopping: monitor={monitor}, mode=min")

        # æ¨¡å‹ä¿å­˜
        if checkpoint_config.get('save_best', True):
            model_checkpoint = ModelCheckpoint(
                save_dir=self.checkpoint_manager.checkpoint_dir,
                monitor='val_loss',  # é¢„è®­ç»ƒå›ºå®šç›‘æ§ val_loss
                mode='min',  # é¢„è®­ç»ƒå›ºå®šä¸º min æ¨¡å¼
                save_frequency=checkpoint_config.get('save_frequency', 5),
                keep_last_n=checkpoint_config.get('keep_last_n', 3)
            )
            self.add_callback(model_checkpoint)
            print(f"  âœ… ModelCheckpoint: monitor=val_loss, mode=min")

    def _plot_curves(self, history: Dict):
        """
        ğŸ”§ æ–°å¢æ–¹æ³•: ç»˜åˆ¶è®­ç»ƒæ›²çº¿

        å¯¹äºå¯¹æ¯”å­¦ä¹ ,æˆ‘ä»¬åªæœ‰loss,æ²¡æœ‰accuracy

        Args:
            history: è®­ç»ƒå†å²,åŒ…å« train_loss, val_loss ç­‰
        """
        print("\nç»˜åˆ¶è®­ç»ƒæ›²çº¿...")

        # ç¡®ä¿å¯è§†åŒ–ç›®å½•å­˜åœ¨
        vis_dir = Path(self.experiment_dir) / 'visualizations'
        vis_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºå¯è§†åŒ–å™¨
        visualizer = TrainingVisualizer(save_dir=str(vis_dir))

        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        if 'train_loss' in history and 'val_loss' in history:
            visualizer.plot_loss_curves(
                train_loss=history['train_loss'],
                val_loss=history['val_loss'],
                title='Contrastive Learning Loss',
                save_name='contrastive_loss_curves.png',
                show=False
            )
            print(f"  âœ“ æŸå¤±æ›²çº¿å·²ä¿å­˜: {vis_dir / 'contrastive_loss_curves.png'}")

        # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿
        if 'learning_rate' in history and len(history['learning_rate']) > 0:
            visualizer.plot_learning_rate(
                learning_rates=history['learning_rate'],
                title='Learning Rate Schedule',
                save_name='learning_rate.png',
                show=False
            )
            print(f"  âœ“ å­¦ä¹ ç‡æ›²çº¿å·²ä¿å­˜: {vis_dir / 'learning_rate.png'}")

        print("âœ“ è®­ç»ƒæ›²çº¿ç»˜åˆ¶å®Œæˆ")

    def save_pretrained_weights(self, save_path: str):
        """
        ä¿å­˜é¢„è®­ç»ƒæƒé‡

        åªä¿å­˜æ¨¡å‹çš„state_dict,ç”¨äºåç»­å¾®è°ƒé˜¶æ®µåŠ è½½
        æ³¨æ„: ä¼šæ’é™¤projection_headçš„æƒé‡(å› ä¸ºå¾®è°ƒä¸éœ€è¦)

        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        print(f"\nä¿å­˜é¢„è®­ç»ƒæƒé‡åˆ°: {save_path}")

        # è·å–å®Œæ•´çš„state_dict
        full_state_dict = self.model.state_dict()

        # åªä¿ç•™backboneæƒé‡,æ’é™¤projection_headå’Œclassifier
        pretrained_dict = {}
        excluded_keys = []

        for k, v in full_state_dict.items():
            # æ’é™¤æŠ•å½±å¤´å’Œåˆ†ç±»å™¨
            if 'projection_head' in k or 'classifier' in k:
                excluded_keys.append(k)
                continue
            pretrained_dict[k] = v

        # ä¿å­˜
        torch.save({
            'model_state_dict': pretrained_dict,
            'epoch': self.current_epoch,
            'temperature': self.temperature,
            'loss_type': self.loss_type
        }, save_path)

        print(f"âœ“ é¢„è®­ç»ƒæƒé‡ä¿å­˜æˆåŠŸ")
        print(f"  - ä¿å­˜å‚æ•°: {len(pretrained_dict)} ä¸ª")
        print(f"  - æ’é™¤å‚æ•°: {len(excluded_keys)} ä¸ª")
        if excluded_keys:
            unique_layers = set([k.split('.')[0] for k in excluded_keys])
            print(f"  - æ’é™¤çš„å±‚: {', '.join(unique_layers)}")


if __name__ == '__main__':
    """æµ‹è¯•ä»£ç """
    import sys
    import os
    from pathlib import Path

    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root))

    print("=" * 80)
    print("æµ‹è¯•ContrastiveTrainer (ä¿®å¤ç‰ˆ)")
    print("=" * 80)

    # åˆ›å»ºç®€å•çš„æµ‹è¯•
    print("\nâœ“ ContrastiveTraineræ¨¡å—åŠ è½½æˆåŠŸ")
    print("âœ“ å·²æ·»åŠ  _plot_curves æ–¹æ³•")
    print("âœ“ å·²æ·»åŠ  save_pretrained_weights æ–¹æ³•")

    print("\n" + "=" * 80)
    print("âœ… ä¿®å¤éªŒè¯é€šè¿‡!")
    print("=" * 80)
