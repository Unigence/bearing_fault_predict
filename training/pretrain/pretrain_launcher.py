"""
é¢„è®­ç»ƒå¯åŠ¨è„šæœ¬
"""
import torch
import os
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import create_model
from datasets import ContrastiveDataset
from torch.utils.data import DataLoader
from augmentation import ContrastiveAugmentation
from training.pretrain.contrastive_trainer import ContrastiveTrainer
from training.optimizer_factory import create_optimizer_from_config
from training.scheduler_factory import create_scheduler_from_config
from utils.config_parser import ModelConfigParser, TrainConfigParser, AugmentationConfigParser
from utils.seed import set_seed


def create_contrastive_dataloaders(
    data_config: dict,
    aug_config: dict,
    batch_size: int
):
    """
    åˆ›å»ºå¯¹æ¯”å­¦ä¹ æ•°æ®åŠ è½½å™¨

    ğŸ”§ ä¿®å¤: è®­ç»ƒé›†ä½¿ç”¨å¼ºå¢å¼º,éªŒè¯é›†ä½¿ç”¨å¼±å¢å¼º

    Args:
        data_config: æ•°æ®é…ç½®
        aug_config: å¢å¼ºé…ç½®
        batch_size: batchå¤§å°

    Returns:
        train_loader, val_loader
    """
    # ğŸ”§ è®­ç»ƒé›†: åˆ›å»ºå¼ºå¯¹æ¯”å­¦ä¹ å¢å¼º
    train_aug = ContrastiveAugmentation(
        strong_aug_prob=aug_config.get('strong_aug_prob', 0.5)
    )

    # ğŸ”§ éªŒè¯é›†: åˆ›å»ºå¼±å¯¹æ¯”å­¦ä¹ å¢å¼ºï¼ˆç”¨äºæ›´ç¨³å®šçš„è¯„ä¼°ï¼‰
    val_aug = ContrastiveAugmentation(
        strong_aug_prob=0.0  # éªŒè¯é›†åªä½¿ç”¨åŸºç¡€å¢å¼º,ä¸ä½¿ç”¨å¼ºå¢å¼º
    )

    # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    train_dataset = ContrastiveDataset(
        data_dir=data_config.get('train_dir', 'raw_datasets/train'),
        window_size=data_config.get('window_size', 512),
        window_step=data_config.get('window_step', 256),
        mode='train',
        fold=data_config.get('current_fold', 0),
        n_folds=data_config.get('n_folds', 5),
        timefreq_method=data_config.get('timefreq_method', 'stft'),
        augmentation=train_aug,  # ä½¿ç”¨å¼ºå¢å¼º
        cache_data=data_config.get('cache_data', True)
    )

    # ğŸ”§ åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆä½¿ç”¨å¼±å¢å¼ºï¼‰
    val_dataset = ContrastiveDataset(
        data_dir=data_config.get('train_dir', 'raw_datasets/train'),
        window_size=data_config.get('window_size', 512),
        window_step=data_config.get('window_step', 256),
        mode='val',
        fold=data_config.get('current_fold', 0),
        n_folds=data_config.get('n_folds', 5),
        timefreq_method=data_config.get('timefreq_method', 'stft'),
        augmentation=val_aug,  # ğŸ”§ ä½¿ç”¨å¼±å¢å¼ºè€Œéå¼ºå¢å¼º
        cache_data=data_config.get('cache_data', True)
    )

    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=data_config.get('num_workers', 4),
        pin_memory=data_config.get('pin_memory', True),
        drop_last=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=data_config.get('num_workers', 4),
        pin_memory=data_config.get('pin_memory', True)
    )

    print(f"  âœ… è®­ç»ƒé›†ä½¿ç”¨å¼ºå¢å¼º(prob={aug_config.get('strong_aug_prob', 0.5)})")
    print(f"  âœ… éªŒè¯é›†ä½¿ç”¨å¼±å¢å¼º(prob=0.0, ä»…åŸºç¡€å˜æ¢)")

    return train_loader, val_loader


def launch_pretrain(
    model_config: ModelConfigParser,
    train_config: TrainConfigParser,
    aug_config: AugmentationConfigParser,
    experiment_name: str = None
):
    """
    å¯åŠ¨é¢„è®­ç»ƒ (æ”¯æŒ k-fold äº¤å‰éªŒè¯)

    Args:
        model_config: æ¨¡å‹é…ç½®è§£æå™¨
        train_config: è®­ç»ƒé…ç½®è§£æå™¨
        aug_config: å¢å¼ºé…ç½®è§£æå™¨
        experiment_name: å®éªŒåç§°

    Returns:
        pretrained_model: é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæœ€åä¸€ä¸ª fold çš„æ¨¡å‹ï¼‰
        experiment_dir: å®éªŒç›®å½•
        pretrained_weights_path: é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆæœ€ä½³ fold çš„æƒé‡ï¼‰
    """
    # è®¾ç½®éšæœºç§å­
    seed = train_config.get_seed()
    set_seed(seed)

    # è·å–è®¾å¤‡
    device = train_config.get_device()
    if not torch.cuda.is_available() and device == 'cuda':
        print("âš ï¸  CUDAä¸å¯ç”¨,ä½¿ç”¨CPUè®­ç»ƒ")
        device = 'cpu'

    print("=" * 80)
    print("å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ (K-Fold äº¤å‰éªŒè¯)")
    print("=" * 80)
    print(f"è®¾å¤‡: {device}")
    print(f"éšæœºç§å­: {seed}")

    # åˆ›å»ºå®éªŒç›®å½•
    if experiment_name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        experiment_name = f"pretrain_{timestamp}"

    experiment_base = train_config.get('experiment.save_dir', 'experiments/runs')
    experiment_dir = Path(experiment_base) / experiment_name
    experiment_dir.mkdir(parents=True, exist_ok=True)

    print(f"å®éªŒç›®å½•: {experiment_dir}")

    # è·å–é…ç½®
    pretrain_params = train_config.get_pretrain_params()
    data_params = train_config.get_data_params()
    aug_params = aug_config.get_contrastive_aug_params()

    # è·å– k-fold é…ç½®
    n_folds = data_params.get('n_folds', 5)
    print(f"\nä½¿ç”¨ {n_folds}-Fold äº¤å‰éªŒè¯")

    # å­˜å‚¨æ¯ä¸ª fold çš„ç»“æœ
    fold_results = []
    best_fold = None
    best_val_loss = float('inf')

    # å¾ªç¯éå†æ‰€æœ‰ folds
    for fold in range(n_folds):
        print("\n" + "=" * 80)
        print(f"Fold {fold + 1}/{n_folds}")
        print("=" * 80)

        # ä¸ºæ¯ä¸ª fold è®¾ç½®éšæœºç§å­ï¼ˆä¿è¯å¯å¤ç°æ€§ï¼‰
        set_seed(seed + fold)

        # åˆ›å»º fold ç‰¹å®šçš„å®éªŒç›®å½•
        fold_dir = experiment_dir / f"fold_{fold}"
        fold_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ¨¡å‹
        print(f"\nåˆ›å»ºæ¨¡å‹ (Fold {fold + 1})...")
        model_params = model_config.get_model_params()
        model = create_model(**model_params, enable_contrastive=True)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯ï¼ˆä»…ç¬¬ä¸€ä¸ª foldï¼‰
        if fold == 0:
            param_dict = model.count_parameters()
            print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            print(f"  - é…ç½®: {model_params['config']}")
            print(f"  - æ€»å‚æ•°: {param_dict['total']:,}")
            print(f"  - å¯è®­ç»ƒå‚æ•°: {param_dict['trainable']:,}")
            print(f"  - æŠ•å½±å¤´å‚æ•°: {param_dict.get('projection_head', 0):,}")

        # æ›´æ–°å½“å‰ fold
        data_params['current_fold'] = fold

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print(f"\nåˆ›å»ºæ•°æ®åŠ è½½å™¨ (Fold {fold + 1})...")
        train_loader, val_loader = create_contrastive_dataloaders(
            data_config=data_params,
            aug_config=aug_params,
            batch_size=pretrain_params['batch_size']
        )
        print(f"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"  - è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
        print(f"  - éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
        print(f"  - Batch size: {pretrain_params['batch_size']}")

        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = create_optimizer_from_config(
            model,
            pretrain_params['optimizer']
        )

        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler, needs_metric = create_scheduler_from_config(
            optimizer,
            pretrain_params['scheduler'],
            total_epochs=pretrain_params['epochs']
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        print(f"\nåˆ›å»ºè®­ç»ƒå™¨ (Fold {fold + 1})...")
        trainer = ContrastiveTrainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            optimizer=optimizer,
            scheduler=scheduler,
            loss_type=pretrain_params['loss'].get('type', 'ntxent'),
            temperature=pretrain_params['loss'].get('temperature', 0.07),
            device=device,
            experiment_dir=str(fold_dir),
            use_amp=train_config.use_amp(),
            gradient_clip_max_norm=pretrain_params['gradient_clip'].get('max_norm', 1.0)
        )

        # è®¾ç½®callbacks
        trainer.setup_callbacks(
            early_stopping_config=pretrain_params.get('early_stopping', {}),
            checkpoint_config=model_config.get_checkpoint_params()
        )

        # å¼€å§‹è®­ç»ƒ
        print(f"\nå¼€å§‹è®­ç»ƒ Fold {fold + 1}...")
        trainer.train(
            epochs=pretrain_params['epochs'],
            log_interval=train_config.get('experiment.logging.log_interval', 10),
            save_config={
                'model': model_config.to_dict(),
                'train': train_config.to_dict(),
                'augmentation': aug_config.to_dict(),
                'fold': fold
            }
        )

        # ä¿å­˜å½“å‰ fold çš„æƒé‡
        fold_weights_path = fold_dir / 'pretrained_weights.pth'
        trainer.save_pretrained_weights(str(fold_weights_path))

        # è·å–æœ€ä½³éªŒè¯æŸå¤±
        best_metric_value, best_epoch = trainer.metrics_tracker.get_best_metric('val_loss', mode='min')

        # è®°å½• fold ç»“æœ
        fold_results.append({
            'fold': fold,
            'best_val_loss': best_metric_value,
            'best_epoch': best_epoch,
            'weights_path': fold_weights_path
        })

        print(f"\nâœ“ Fold {fold + 1} å®Œæˆ")
        print(f"  - æœ€ä½³éªŒè¯æŸå¤±: {best_metric_value:.4f} (Epoch {best_epoch})")
        print(f"  - æƒé‡è·¯å¾„: {fold_weights_path}")

        # æ›´æ–°æœ€ä½³ fold
        if best_metric_value < best_val_loss:
            best_val_loss = best_metric_value
            best_fold = fold
            best_model = model

    # æ‰“å°æ‰€æœ‰ folds çš„æ€»ç»“
    print("\n" + "=" * 80)
    print("K-Fold äº¤å‰éªŒè¯ç»“æœæ€»ç»“")
    print("=" * 80)
    for result in fold_results:
        mark = "â­ " if result['fold'] == best_fold else "  "
        print(f"{mark}Fold {result['fold'] + 1}: val_loss = {result['best_val_loss']:.4f} (epoch {result['best_epoch']})")

    # è®¡ç®—å¹³å‡å’Œæ ‡å‡†å·®
    val_losses = [r['best_val_loss'] for r in fold_results]
    import numpy as np
    mean_val_loss = np.mean(val_losses)
    std_val_loss = np.std(val_losses)

    print(f"\néªŒè¯æŸå¤±ç»Ÿè®¡:")
    print(f"  - å¹³å‡å€¼: {mean_val_loss:.4f}")
    print(f"  - æ ‡å‡†å·®: {std_val_loss:.4f}")
    print(f"  - æœ€ä½³ Fold: {best_fold + 1} (val_loss = {best_val_loss:.4f})")

    # å¤åˆ¶æœ€ä½³ fold çš„æƒé‡åˆ°ä¸»ç›®å½•
    pretrained_weights_path = experiment_dir / 'pretrained_weights.pth'
    import shutil
    shutil.copy(fold_results[best_fold]['weights_path'], pretrained_weights_path)

    print("\n" + "=" * 80)
    print("âœ“ é¢„è®­ç»ƒå®Œæˆ!")
    print("=" * 80)
    print(f"é¢„è®­ç»ƒæƒé‡ (æœ€ä½³ Fold): {pretrained_weights_path}")
    print(f"å®éªŒç›®å½•: {experiment_dir}")
    print(f"å„ Fold ç»“æœä¿å­˜åœ¨: {experiment_dir}/fold_*")

    return best_model, experiment_dir, pretrained_weights_path


if __name__ == '__main__':
    """ç‹¬ç«‹è¿è¡Œé¢„è®­ç»ƒ"""
    print("=" * 70)
    print("é¢„è®­ç»ƒå¯åŠ¨è„šæœ¬æµ‹è¯•ï¼ˆå·²ä¿®å¤ç‰ˆæœ¬ï¼‰")
    print("=" * 70)

    print("\nâœ… ä¿®å¤è¯´æ˜:")
    print("  4. éªŒè¯é›†ä½¿ç”¨å¼±å¢å¼º")
    print("     - è®­ç»ƒé›†: strong_aug_prob=0.5 (å¼ºå¢å¼º)")
    print("     - éªŒè¯é›†: strong_aug_prob=0.0 (å¼±å¢å¼º/åŸºç¡€å˜æ¢)")
    print("     - åŸå› : éªŒè¯é›†éœ€è¦æ›´ç¨³å®šçš„è¯„ä¼°,ä¸åº”ä½¿ç”¨å¼ºå¢å¼º")
    print("     - æ³¨æ„: å¯¹æ¯”å­¦ä¹ ä»éœ€è¦ä¸¤ä¸ªè§†å›¾,ä½†å¢å¼ºå¼ºåº¦é™ä½")

    print("\nâœ“ é¢„è®­ç»ƒå¯åŠ¨è„šæœ¬æ¨¡å—åŠ è½½æˆåŠŸ")
    print("=" * 70)
